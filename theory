
Основные концепции и терминология в области искусственного интеллекта (ИИ) охватывают широкий спектр тем, связанных с созданием и использованием систем, которые могут выполнять задачи, требующие интеллекта.

Вот основные термины и концепции, которые вам нужно знать:

### Основные концепции

1. **Искусственный интеллект (ИИ)**:
   - Общий термин, обозначающий машины или системы, которые могут выполнять задачи, требующие человеческого интеллекта, такие как понимание естественного языка, распознавание изображений, принятие решений и т. д.

2. **Машинное обучение (ML)**:
   - Подмножество ИИ, в котором используются алгоритмы, позволяющие системам обучаться и улучшать свои функции на основе данных. В машинном обучении выделяют несколько типов:
     - **Обучение с учителем**: Алгоритм обучается на размеченных данных, где каждому входу соответствует правильный выход.
     - **Обучение без учителя**: Алгоритм анализирует данные без предварительной разметки, выявляя скрытые структуры или закономерности.
     - **Обучение с подкреплением**: Алгоритм обучается на основе обратной связи от среды, получая награды или штрафы за свои действия.

3. **Глубокое обучение (DL)**:
   - Подмножество машинного обучения, основанное на искусственных нейронных сетях с большим количеством слоев (глубокие нейронные сети). Оно используется для задач, таких как обработка изображений, распознавание речи и понимание текста.

4. **Обработка естественного языка (NLP)**:
   - Подобласть ИИ, занимающаяся взаимодействием между компьютерами и человеческими языками. Задачи включают понимание, генерацию и перевод текста.

### Основная терминология

1. **Алгоритмы**:
   - Последовательность инструкций, используемых для решения задач или выполнения вычислений.

2. **Данные и датасеты**:
   - Информация, на которой обучаются и тестируются алгоритмы ИИ. Датасеты могут быть размеченными (с метками) или неразмеченными.

3. **Нейронные сети**:
   - Модели, вдохновленные биологическими нейронными сетями, состоящие из узлов (нейронов) и соединений (синапсов). Включают слои: входной, скрытые и выходной.

4. **Гиперпараметры**:
   - Параметры модели, которые настраиваются до обучения и не изменяются в процессе обучения, такие как скорость обучения, число слоев и число нейронов в каждом слое.

5. **Обратное распространение (Backpropagation)**:
   - Алгоритм для обучения нейронных сетей, который использует градиентный спуск для минимизации ошибки, корректируя веса сети.

6. **Градиентный спуск**:
   - Оптимизационный метод, используемый для минимизации функции ошибки путем обновления параметров модели в направлении, противоположном градиенту.

7. **Регуляризация**:
   - Техники, используемые для предотвращения переобучения модели, такие как L1 и L2 регуляризация, dropout и другие.

8. **Переобучение и недообучение**:
   - **Переобучение (Overfitting)**: Модель хорошо обучается на тренировочных данных, но плохо обобщает на новых данных.
   - **Недообучение (Underfitting)**: Модель не может хорошо обучиться даже на тренировочных данных.

9. **Точность (Accuracy)** и **погрешность (Error)**:
   - Метрики, используемые для оценки производительности модели. Точность – доля правильно предсказанных случаев, погрешность – разница между предсказанным и истинным значением.

10. **Конволюционные нейронные сети (CNN)**:
    - Специализированные нейронные сети для обработки данных с сетчатой топологией, таких как изображения. Используют сверточные слои для автоматического выделения признаков.

11. **Рекуррентные нейронные сети (RNN)**:
    - Нейронные сети, предназначенные для работы с последовательными данными, такими как временные ряды и текст. Включают механизмы, такие как LSTM и GRU, для хранения долгосрочных зависимостей.

12. **Автокодировщики (Autoencoders)**:
    - Нейронные сети, используемые для обучения эффективного кодирования данных. Состоят из двух частей: кодировщика и декодировщика.

13. **Генеративно-состязательные сети (GAN)**:
    - Модели, состоящие из двух нейронных сетей: генератора и дискриминатора, которые обучаются одновременно. Генератор создает фальшивые данные, дискриминатор пытается отличить фальшивые данные от настоящих

14. **Трансформеры (Transformers)**:
    - Архитектуры для обработки последовательных данных, особенно в NLP, которые используют механизм внимания для параллельной обработки данных.
